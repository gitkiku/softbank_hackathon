{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZ98Gn1Pw1TO4b2kM3406o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitkiku/softbank_hackathon/blob/main/cafe_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDr7Q1eWFfdj",
        "outputId": "82e66dfd-5b58-4f08-9ef1-4c49192b6074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install line-bot-sdk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uD1JCwgCGZR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RR3dPzJKLubH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# 収集先URL\n",
        "url = \"https://www.sensor-ryukoku.com/index.html\"\n",
        "response = requests.get(url)\n",
        "response.encoding = response.apparent_encoding\n",
        "\n",
        "# レスポンスの内容をBeautifulSoupで解析\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# 特定のIDを持つ要素をすべて取得\n",
        "target_ids = ['tab01_content', 'tab02_content', 'tab03_content']  # ここに探したいIDを指定\n",
        "\n",
        "# タイトルと混雑状況を格納するリスト\n",
        "titles = []\n",
        "statuses = []\n",
        "\n",
        "# タイトルを取得\n",
        "target_class = 'cafe_title'  # ここに探したいクラス名を指定\n",
        "for target_id in target_ids:\n",
        "    # 特定のIDを持つ要素を取得\n",
        "    element = soup.find(id=target_id)\n",
        "    if element:\n",
        "        # 下位層の特定のクラスを持つdiv要素をすべて取得\n",
        "        sub_divs = element.find_all('div', class_=target_class)\n",
        "        for sub_div in sub_divs:\n",
        "            # テキストを取得し、余分な空白を正規表現で取り除く\n",
        "            text = sub_div.get_text(separator=' ')\n",
        "            clean_text = re.sub(r'\\s+', ' ', text).strip()\n",
        "            titles.append(clean_text)\n",
        "\n",
        "# 混雑状況を取得\n",
        "target_class = 'cafe_crowded'  # ここに探したいクラス名を指定\n",
        "for target_id in target_ids:\n",
        "    # 特定のIDを持つ要素を取得\n",
        "    element = soup.find(id=target_id)\n",
        "    if element:\n",
        "        # 下位層の特定のクラスを持つdiv要素をすべて取得\n",
        "        sub_divs = element.find_all('div', class_=target_class)\n",
        "        for sub_div in sub_divs:\n",
        "            # 下位層のimg要素をすべて取得\n",
        "            imgs = sub_div.find_all('img')\n",
        "            for img in imgs:\n",
        "                src = img['src']\n",
        "                if src == '/img/status_1.png':\n",
        "                    statuses.append(\"余裕あり\")\n",
        "                elif src == '/img/status_2.png':\n",
        "                    statuses.append(\"不明\")\n",
        "                elif src == '/img/status_3.png':\n",
        "                    statuses.append(\"不明\")\n",
        "                elif src == '/img/status_4.png':\n",
        "                    statuses.append(\"不明\")\n",
        "                elif src == '/img/status_5.png':\n",
        "                    statuses.append(\"CLOSED\")\n",
        "                elif src == '/img/status_.png':\n",
        "                    statuses.append(\"メンテナンス中\")\n",
        "                else:\n",
        "                    statuses.append(src)  # 既知のsrc値以外の場合\n",
        "\n",
        "# キャンパスごとに分けて表示\n",
        "fukakusa_titles = titles[:8]\n",
        "fukakusa_statuses = statuses[:8]\n",
        "\n",
        "seta_titles = titles[8:14]\n",
        "seta_statuses = statuses[8:14]\n",
        "\n",
        "omiya_titles = titles[14:]\n",
        "omiya_statuses = statuses[14:]\n",
        "\n",
        "print(\"深草キャンパス:\")\n",
        "for title, status in zip(fukakusa_titles, fukakusa_statuses):\n",
        "    print(f'{title}: {status}')\n",
        "\n",
        "print(\"\\n瀬田キャンパス:\")\n",
        "for title, status in zip(seta_titles, seta_statuses):\n",
        "    print(f'{title}: {status}')\n",
        "\n",
        "print(\"\\n大宮キャンパス:\")\n",
        "for title, status in zip(omiya_titles, omiya_statuses):\n",
        "    print(f'{title}: {status}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pInch8oo0OEC",
        "outputId": "28f7bf1c-ca6b-4fa6-c4d4-ec6f516cc5f9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "深草キャンパス:\n",
            "22号館地下食堂: 余裕あり\n",
            "3号館地下食堂: CLOSED\n",
            "4号館地下食堂: CLOSED\n",
            "Cafe樹林: CLOSED\n",
            "Café Ryukoku & （成就館）: CLOSED\n",
            "生協深草ショップ R-Uni: CLOSED\n",
            "スターバックス: 余裕あり\n",
            "セブンイレブン: メンテナンス中\n",
            "\n",
            "瀬田キャンパス:\n",
            "青志館食堂: 余裕あり\n",
            "パフェ工房 （青志館2F）: 余裕あり\n",
            "生協瀬田ショップ SMILE: 余裕あり\n",
            "不二家食堂レストラン （青雲館1階）: CLOSED\n",
            "不二家食堂オムライス専門店（青雲館2階）: CLOSED\n",
            "ファミリーマート: CLOSED\n",
            "\n",
            "大宮キャンパス:\n",
            "食堂（清和館1F）: CLOSED\n",
            "生協大宮店購買部 （清和館BF）: CLOSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# 収集先URL\n",
        "url = \"https://www.sensor-ryukoku.com/index.html\"\n",
        "response = requests.get(url)\n",
        "response.encoding = response.apparent_encoding\n",
        "\n",
        "# レスポンスの内容をBeautifulSoupで解析\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# 特定のIDを持つ要素をすべて取得\n",
        "target_ids = ['tab01_content', 'tab02_content', 'tab03_content']  # ここに探したいIDを指定\n",
        "\n",
        "# タイトルと混雑状況を格納するリスト\n",
        "titles = []\n",
        "statuses = []\n",
        "\n",
        "# タイトルを取得\n",
        "target_class = 'cafe_title'  # ここに探したいクラス名を指定\n",
        "for target_id in target_ids:\n",
        "    # 特定のIDを持つ要素を取得\n",
        "    element = soup.find(id=target_id)\n",
        "    if element:\n",
        "        # 下位層の特定のクラスを持つdiv要素をすべて取得\n",
        "        sub_divs = element.find_all('div', class_=target_class)\n",
        "        for sub_div in sub_divs:\n",
        "            # テキストを取得し、余分な空白を正規表現で取り除く\n",
        "            text = sub_div.get_text(separator=' ')\n",
        "            clean_text = re.sub(r'\\s+', ' ', text).strip()\n",
        "            titles.append(clean_text)\n",
        "\n",
        "# 混雑状況を取得\n",
        "target_class = 'cafe_crowded'  # ここに探したいクラス名を指定\n",
        "for target_id in target_ids:\n",
        "    # 特定のIDを持つ要素を取得\n",
        "    element = soup.find(id=target_id)\n",
        "    if element:\n",
        "        # 下位層の特定のクラスを持つdiv要素をすべて取得\n",
        "        sub_divs = element.find_all('div', class_=target_class)\n",
        "        for sub_div in sub_divs:\n",
        "            # 下位層のimg要素をすべて取得\n",
        "            imgs = sub_div.find_all('img')\n",
        "            for img in imgs:\n",
        "                src = img['src']\n",
        "                if src == '/img/status_1.png':\n",
        "                    statuses.append(\"余裕あり\")\n",
        "                elif src == '/img/status_2.png':\n",
        "                    statuses.append(\"不明\")\n",
        "                elif src == '/img/status_3.png':\n",
        "                    statuses.append(\"不明\")\n",
        "                elif src == '/img/status_4.png':\n",
        "                    statuses.append(\"不明\")\n",
        "                elif src == '/img/status_5.png':\n",
        "                    statuses.append(\"CLOSED\")\n",
        "                elif src == '/img/status_.png':\n",
        "                    statuses.append(\"メンテナンス中\")\n",
        "                else:\n",
        "                    statuses.append(src)  # 既知のsrc値以外の場合\n",
        "\n",
        "# タイトルと混雑状況をペアにして表示\n",
        "for title, status in zip(titles, statuses):\n",
        "    print(f'{title}: {status}')\n",
        "\n",
        "print(titles)\n",
        "print(statuses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4zyoJiPzZIN",
        "outputId": "148f00f3-cf3e-407e-8dee-3e9989a96588"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22号館地下食堂: 余裕あり\n",
            "3号館地下食堂: CLOSED\n",
            "4号館地下食堂: CLOSED\n",
            "Cafe樹林: CLOSED\n",
            "Café Ryukoku & （成就館）: CLOSED\n",
            "生協深草ショップ R-Uni: CLOSED\n",
            "スターバックス: 余裕あり\n",
            "セブンイレブン: メンテナンス中\n",
            "青志館食堂: 余裕あり\n",
            "パフェ工房 （青志館2F）: 余裕あり\n",
            "生協瀬田ショップ SMILE: 余裕あり\n",
            "不二家食堂レストラン （青雲館1階）: CLOSED\n",
            "不二家食堂オムライス専門店（青雲館2階）: CLOSED\n",
            "ファミリーマート: CLOSED\n",
            "食堂（清和館1F）: CLOSED\n",
            "生協大宮店購買部 （清和館BF）: CLOSED\n",
            "['22号館地下食堂', '3号館地下食堂', '4号館地下食堂', 'Cafe樹林', 'Café Ryukoku & （成就館）', '生協深草ショップ R-Uni', 'スターバックス', 'セブンイレブン', '青志館食堂', 'パフェ工房 （青志館2F）', '生協瀬田ショップ SMILE', '不二家食堂レストラン （青雲館1階）', '不二家食堂オムライス専門店（青雲館2階）', 'ファミリーマート', '食堂（清和館1F）', '生協大宮店購買部 （清和館BF）']\n",
            "['余裕あり', 'CLOSED', 'CLOSED', 'CLOSED', 'CLOSED', 'CLOSED', '余裕あり', 'メンテナンス中', '余裕あり', '余裕あり', '余裕あり', 'CLOSED', 'CLOSED', 'CLOSED', 'CLOSED', 'CLOSED']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from linebot import LineBotApi\n",
        "from linebot.models import TextSendMessage\n",
        "\n",
        "# 収集先URL\n",
        "url = \"https://www.sensor-ryukoku.com/index.html\"\n",
        "response = requests.get(url)\n",
        "response.encoding = response.apparent_encoding\n",
        "\n",
        "# レスポンスの内容をBeautifulSoupで解析\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# 特定のIDを持つ要素をすべて取得\n",
        "target_ids = ['tab01_content', 'tab02_content', 'tab03_content']  # ここに探したいIDを指定\n",
        "\n",
        "# タイトルと混雑状況を格納するリスト\n",
        "titles = []\n",
        "statuses = []\n",
        "\n",
        "# タイトルを取得\n",
        "target_class = 'cafe_title'  # ここに探したいクラス名を指定\n",
        "for target_id in target_ids:\n",
        "    # 特定のIDを持つ要素を取得\n",
        "    element = soup.find(id=target_id)\n",
        "    if element:\n",
        "        # 下位層の特定のクラスを持つdiv要素をすべて取得\n",
        "        sub_divs = element.find_all('div', class_=target_class)\n",
        "        for sub_div in sub_divs:\n",
        "            # テキストを取得し、余分な空白を正規表現で取り除く\n",
        "            text = sub_div.get_text(separator=' ')\n",
        "            clean_text = re.sub(r'\\s+', ' ', text).strip()\n",
        "            titles.append(clean_text)\n",
        "\n",
        "# 混雑状況を取得\n",
        "target_class = 'cafe_crowded'  # ここに探したいクラス名を指定\n",
        "for target_id in target_ids:\n",
        "    # 特定のIDを持つ要素を取得\n",
        "    element = soup.find(id=target_id)\n",
        "    if element:\n",
        "        # 下位層の特定のクラスを持つdiv要素をすべて取得\n",
        "        sub_divs = element.find_all('div', class_=target_class)\n",
        "        for sub_div in sub_divs:\n",
        "            # 下位層のimg要素をすべて取得\n",
        "            imgs = sub_div.find_all('img')\n",
        "            for img in imgs:\n",
        "                src = img['src']\n",
        "                if src == '/img/status_1.png':\n",
        "                    statuses.append(\"余裕あり\")\n",
        "                elif src == '/img/status_2.png':\n",
        "                    statuses.append(\"不明\")\n",
        "                elif src == '/img/status_3.png':\n",
        "                    statuses.append(\"不明\")\n",
        "                elif src == '/img/status_4.png':\n",
        "                    statuses.append(\"不明\")\n",
        "                elif src == '/img/status_5.png':\n",
        "                    statuses.append(\"CLOSED\")\n",
        "                elif src == '/img/status_.png':\n",
        "                    statuses.append(\"メンテナンス中\")\n",
        "                else:\n",
        "                    statuses.append(src)  # 既知のsrc値以外の場合\n",
        "\n",
        "# キャンパスごとに分けて表示\n",
        "fukakusa_titles = titles[:8]\n",
        "fukakusa_statuses = statuses[:8]\n",
        "\n",
        "seta_titles = titles[8:14]\n",
        "seta_statuses = statuses[8:14]\n",
        "\n",
        "omiya_titles = titles[14:]\n",
        "omiya_statuses = statuses[14:]\n",
        "\n",
        "def format_campus_data(titles, statuses):\n",
        "    return \"\\n\".join([f'{title}: {status}' for title, status in zip(titles, statuses)])\n",
        "\n",
        "fukakusa_data = format_campus_data(fukakusa_titles, fukakusa_statuses)\n",
        "seta_data = format_campus_data(seta_titles, seta_statuses)\n",
        "omiya_data = format_campus_data(omiya_titles, omiya_statuses)\n",
        "\n",
        "# LINE API設定\n",
        "line_bot_api = LineBotApi('YOUR_CHANNEL_ACCESS_TOKEN')\n",
        "\n",
        "# メッセージを送信\n",
        "messages = [\n",
        "    TextSendMessage(text=\"深草キャンパス:\\n\" + fukakusa_data),\n",
        "    TextSendMessage(text=\"瀬田キャンパス:\\n\" + seta_data),\n",
        "    TextSendMessage(text=\"大宮キャンパス:\\n\" + omiya_data)\n",
        "]\n",
        "\n",
        "for message in messages:\n",
        "    line_bot_api.push_message('YOUR_USER_ID', message)\n",
        "\n",
        "\n",
        "# YOUR_CHANNEL_ACCESS_TOKEN と YOUR_USER_ID を自身のLINE Developerアカウントで取得したアクセストークンとユーザーIDに置き換える"
      ],
      "metadata": {
        "id": "3GPvygG80zC5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}